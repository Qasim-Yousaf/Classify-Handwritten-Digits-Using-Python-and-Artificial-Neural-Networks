# -*- coding: utf-8 -*-
"""Untitled0.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1QeLQTaDTejeDlz4fzEKPvURkN-Zd_3eo
"""

# Description : this program can classify handWritten digits from [0-9] using MNIST dataset of handwritten images 
pip install tensorflow keras numpy matplotlib

pip install mnist

#import all the packages or dependenices 
import numpy as np # 
import mnist # get data set from  
from keras.models import Sequential # ANN architecture 
from keras.layers import Dense # the layers in ANN
from keras.utils import to_categorical # used to transform the data Ex. 0,1 => 001, 010 binary 
import matplotlib.pyplot as plt # for using the functionality of graph

#Load the data set 
train_images =  mnist.train_images() # return the traing images  
train_labels =  mnist.train_labels() # return the  labels for traning images 
test_images  =  mnist.test_images() # return the testing images from mnist data set 
test_labels  =  mnist.test_labels() # return the testing images label

#Normalize the images , Normalize the pixel value from [0 , 255] to
# [-0.5 , 0.5] to make our network easier to train 
train_images = (train_images/255) - 0.5
test_images = (test_images/255) - 0.5
#Flatten the images, Flatten each 28*28 image into a 784 dimensional vector
#to pass into the neural network 
train_images  = train_images.reshape((-1,784))
test_images = test_images.reshape((-1,784))
#print the images 
print(train_images.shape)# EX. 60000 rows , 784 cols
print(test_images.shape) #Ex. 10000 rows, 784 cols

# Build the model
# 3 layers , 2 layers with 64 neurons and the relu function 
# 1 layers with 10 neurons and softmax function 
model = Sequential()
model.add(Dense(64, activation='relu', input_dim=784 ))
model.add(Dense(64,activation='relu'))
model.add(Dense(10, activation='softmax'))

# compile the model 
# the loss function measures how well the model did on training , and then tries
# to improve on it using  the optimizer
model.compile(
    optimizer = 'adam',
    loss = 'categorical_crossentropy',# (classes that are greater then 2)
    metrics = ['accuracy']
)

#train the model 
model.fit(
    train_images,
    to_categorical(train_labels), # Ex fun rtn 2 it expects [0,0,1,0,0,0,0,0,0,0] it's 10 dimensional Vector 
    epochs = 5,# no of iteration on the data set in order to train the machine 
    batch_size = 3 # the number of samples per gradient update for training
)

# Evaluate the model 
model.evaluate(
    test_images,
    to_categorical(test_labels)
)

#save a model to disk
model.save_weights('model.h5')

#predict on the first 5 test images 
predictions = model.predict(test_images[:5])
print(np.argmax(predictions, axis=1))
print(test_labels[:5])

for i in range(0,5):
  first_image = test_images[i]
  first_image = np.array(first_image, dtype='float')
  pixels = first_image.reshape((28,28))
  plt.imshow(pixels, cmap='gray')
  plt.show()